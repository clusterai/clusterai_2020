{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0_jwpgj0E40"
   },
   "source": [
    "# ClusterAI 2020\n",
    "# Ciencia de Datos - Ingeniería Industrial - UTN BA\n",
    "# clase_XX: Clasifación con Redes Recurrentes\n",
    "### Elaborado por: Nicolás Aguirre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpuM_mWtms9a"
   },
   "source": [
    "### Montar G.Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27761,
     "status": "ok",
     "timestamp": 1606066789196,
     "user": {
      "displayName": "Cluster AI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhieU54HU6iNPdQE71F_fCSeuzFpr1KEUKjfiyo=s64",
      "userId": "16358828312196303941"
     },
     "user_tz": -60
    },
    "id": "vlEdYAC_2oAM",
    "outputId": "e2be437b-b535-41ae-8ed9-9e841b381f18"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2XFOLPDnlnf"
   },
   "source": [
    "A continuacion copiamos los archivos de nuestro G.Drive al directorio de la PC virtual\n",
    "\n",
    "Esto es recomendable para bases de datos grandes, no tanto para este caos en particular, donde los archivos pesan pocos Mb.\n",
    "\n",
    "En otras cirunstancias acceder a videos o imagenes, puede pasa a ser un cuello de botella, ya que la informacion de nuestro G.Drive no esta fisicamente en la misma ubicacion que la notebook.\n",
    "\n",
    "De esta manera, accedemos a los archivos de forma mas rapida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33693,
     "status": "ok",
     "timestamp": 1606066795133,
     "user": {
      "displayName": "Cluster AI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhieU54HU6iNPdQE71F_fCSeuzFpr1KEUKjfiyo=s64",
      "userId": "16358828312196303941"
     },
     "user_tz": -60
    },
    "id": "d91YYtus8OIw"
   },
   "outputs": [],
   "source": [
    "!cp /content/gdrive/My\\ Drive/clusterai_2020/clases/datasets/clase12/*.txt /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGEEQVOqn-AF"
   },
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1904,
     "status": "ok",
     "timestamp": 1606066798488,
     "user": {
      "displayName": "Cluster AI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhieU54HU6iNPdQE71F_fCSeuzFpr1KEUKjfiyo=s64",
      "userId": "16358828312196303941"
     },
     "user_tz": -60
    },
    "id": "gXHl1KW72RhO"
   },
   "outputs": [],
   "source": [
    "#Datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Graficos \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP4NHy3Ipasl"
   },
   "source": [
    "# Experimental Scenario\n",
    "---\n",
    "Data sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine – i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n",
    "\n",
    "The engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective of the competition is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate. Also provided a vector of true Remaining Useful Life (RUL) values for the test data.\n",
    "\n",
    "The data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxEwgiqfoQHx"
   },
   "source": [
    "# Carga de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oovhfaloXyM"
   },
   "source": [
    "Vamos a cargar los datos con Pandas, y a nombrar las columnas para que sea mas facil el manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1606066799678,
     "user": {
      "displayName": "Cluster AI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhieU54HU6iNPdQE71F_fCSeuzFpr1KEUKjfiyo=s64",
      "userId": "16358828312196303941"
     },
     "user_tz": -60
    },
    "id": "huRQhczq2Rhb"
   },
   "outputs": [],
   "source": [
    "#Nombres de las columnas\n",
    "columns_name = ['Unit','Time','OS1','OS2','OS3',\n",
    "                's01','s02','s03','s04','s05','s06','s07','s08','s09','s10',\n",
    "                's11','s12','s13','s14','s15','s16','s17','s18','s19','s20',\n",
    "                's21','s22','s23','Name']\n",
    "\n",
    "features_columns = ['OS1','OS2','OS3',\n",
    "                's01','s02','s03','s04','s05','s06','s07','s08','s09','s10',\n",
    "                's11','s12','s13','s14','s15','s16','s17','s18','s19','s20',\n",
    "                's21']\n",
    "#Nombre de los archivos\n",
    "names = ['FD001','FD002','FD003','FD004']\n",
    "\n",
    "# Definimos los train y test df\n",
    "train_df = pd.DataFrame(columns=columns_name)\n",
    "test_df = pd.DataFrame(columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1509,
     "status": "ok",
     "timestamp": 1606066801707,
     "user": {
      "displayName": "Cluster AI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhieU54HU6iNPdQE71F_fCSeuzFpr1KEUKjfiyo=s64",
      "userId": "16358828312196303941"
     },
     "user_tz": -60
    },
    "id": "akHV9nMr2Rho"
   },
   "outputs": [],
   "source": [
    "for i_name in names:\n",
    "    # Aqui cargamos los .txt en dataframes de pivot\n",
    "    # que luego los pasaremos al dataframe definitivo\n",
    "    df_train_pivot = pd.read_csv('train_'+i_name+'.txt', sep=\" \", header=None)\n",
    "    df_test_pivot = pd.read_csv('test_'+i_name+'.txt', sep=\" \", header=None)\n",
    "    # Creamos la columna Name y guardamos el nombre dle archivo\n",
    "    df_train_pivot['Name'] = i_name\n",
    "    df_test_pivot['Name'] = i_name\n",
    "\n",
    "    #Nombramos las columnas segun el readme.txt\n",
    "    df_train_pivot.columns = columns_name\n",
    "    df_test_pivot.columns = columns_name\n",
    "    \n",
    "    #Juntamos el df-pivot al df general\n",
    "    train_df = train_df.append(df_train_pivot, sort=False)\n",
    "    test_df = test_df.append(df_test_pivot, sort=False)\n",
    "\n",
    "#Reseteamos los indices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9enSJWr2Rhw"
   },
   "outputs": [],
   "source": [
    "#Verifiquemos los NaNs\n",
    "list_df = [train_df,test_df]\n",
    "for i_df in list_df:\n",
    "    # Cantidad de valores nulos ordenados descendentemente\n",
    "    total = i_df.isnull().sum().sort_values(ascending=False)\n",
    "    # Porcetaje de lo que representa para cada columna\n",
    "    percent = (i_df.isnull().sum()/i_df.isnull().count()).sort_values(ascending=False)\n",
    "    # Mostramos los 2 resultados en conjunto.\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    print(missing_data.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8RQGpixpNd4"
   },
   "source": [
    "Vemos que la columas de los sensores 22 y 23 no contienen informacion.\n",
    "Por lo que vamos a eliminarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVSakdVs2Rh8"
   },
   "outputs": [],
   "source": [
    "#Eliminamos columnas con NaN\n",
    "train_df = train_df.drop(columns=['s22','s23'])\n",
    "test_df = test_df.drop(columns=['s22','s23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1HHp3quqaRJ"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEb76u5kpYXI"
   },
   "source": [
    "Vamos a definir una duracion minima que deban tener las series para analizar.\n",
    "\n",
    "Cualquiera que tenga un valor menor, la eliminaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjA9-qoav9gs"
   },
   "outputs": [],
   "source": [
    "# Duracion de las series temporales\n",
    "lim_t = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs4Q4rn52RiH"
   },
   "outputs": [],
   "source": [
    "# Ahora debemos agregar una colunma al train que nos diga el tiempo que falta para la falla, \n",
    "# es decir, el tiempo restante hasta que cada turbina alcance valor maximo en la columa 'Time'.\n",
    "\n",
    "# Definimos la columa RUL (Rest Util Life) y la completamos con NaN\n",
    "train_df['RUL'] = np.nan\n",
    "\n",
    "# Definimos listas vacias auxiliares\n",
    "list_RUL_train = []\n",
    "list_time_train = []\n",
    "indx_delet = []\n",
    "\n",
    "# Para cada unidad de cada archivo primero verificaremos su duracion.\n",
    "# En caso de ser mayor a la duracion minima, guardaremos el tiempo total de la serie temporal.\n",
    "\n",
    "for i_names in names:\n",
    "    # Por archivo\n",
    "    df_pivot1 = train_df[train_df['Name']==i_names]\n",
    "    # Lista de unidade\n",
    "    list_units = df_pivot1['Unit'].unique()\n",
    "    for i_unit in list_units:\n",
    "        # Informacion de la unidad\n",
    "        df_pivot2 = df_pivot1[df_pivot1['Unit']==i_unit]\n",
    "        # Duracion de la serie temporal\n",
    "        len_ts = df_pivot2.shape[0]\n",
    "      \n",
    "        if len_ts<lim_t:\n",
    "          #DURACION INCORRECTA  \n",
    "          # En caso de ser muy corta, guardamos los indices para luego eliminarlos\n",
    "          indx = df_pivot2.index\n",
    "          indx_delet = np.concatenate((np.asarray(indx_delet),indx.values))\n",
    "        else: \n",
    "          #DURACION CORRECTA\n",
    "          # Tiempo maximo de cada unidad\n",
    "          max_RUL = df_pivot2.Time.max()\n",
    "          list_RUL_train.append(max_RUL)\n",
    "          list_time_train.append(len_ts)\n",
    "          # Indices\n",
    "          indx = df_pivot2.index\n",
    "\n",
    "          #Reemplazamos RUL por la diferencia entre el maximo y el tiempo de ciclo\n",
    "          train_df.iloc[indx,-1] = (max_RUL - df_pivot2.Time).values\n",
    "\n",
    "# Convertimos las listas en numpy arrays\n",
    "list_time_train = np.asarray(list_time_train)\n",
    "list_RUL_train = np.asarray(list_RUL_train)\n",
    "\n",
    "# Eliminamos los indices que tenian duraciones menores\n",
    "train_df.drop(index = indx_delet,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_jFKGnuZN5y"
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gETq_mUS2RiP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.kdeplot(list_RUL_train, color= \"Blue\", shade = True)\n",
    "plt.xlabel(\"RUL\",size = 20)\n",
    "plt.ylabel(\"Frecuencia\",size = 20)\n",
    "plt.title('Distribucion de RUL',size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5IGvBBk2RiW"
   },
   "outputs": [],
   "source": [
    "# Ahora para el test set, vamos a adjuntar el dato RUL de cada unidad al dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMVNti0nry-c"
   },
   "source": [
    "A diferencia de la informacion de entrenamiento, para la cual sabemos que la serie temporal termina **justo** en la falla, en los datos de test, nos dan una serie temporal y aparte tenemos los datos de cuanto le faltaba hasta la falla al equipo.\n",
    "\n",
    "Esta informacion separada la vamos a consolidar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOcSzC4g2Rie"
   },
   "outputs": [],
   "source": [
    "# Ahora para el test set, vamos a adjuntar el dato RUL de cada unidad al dataframe.\n",
    "test_df['RUL'] = np.nan\n",
    "list_RUL_test = []\n",
    "list_time_test = []\n",
    "indx_delet = []\n",
    "\n",
    "for i_names in names:\n",
    "    # Por archivo\n",
    "    df_pivot1 = test_df[test_df['Name']==i_names]\n",
    "    # lista de unidade\n",
    "    list_units = df_pivot1['Unit'].unique()\n",
    "    # lista de RUL\n",
    "    df_RUL_pivot = pd.read_csv('RUL_'+i_names+'.txt', sep=\" \", header=None)\n",
    "    for i_unit in list_units:\n",
    "        i_RUL = df_RUL_pivot.iloc[i_unit-1,0]\n",
    "        # Por Unidad\n",
    "        df_pivot2 = df_pivot1[df_pivot1['Unit']==i_unit]\n",
    "        len_ts = df_pivot2.shape[0]\n",
    "        if len_ts<lim_t:\n",
    "          indx = df_pivot2.index\n",
    "          indx_delet = np.concatenate((np.asarray(indx_delet),indx.values))\n",
    "        else:\n",
    "          # Tiempo maximo de cada unidad\n",
    "          i_RUL = df_RUL_pivot.iloc[i_unit-1,0]\n",
    "          list_RUL_test.append(i_RUL)\n",
    "          list_time_test.append(len_ts)\n",
    "          # Indices\n",
    "          indx = df_pivot2.index\n",
    "          # Reemplazamos RUL por la diferencia entre el maximo y el tiempo de ciclo\n",
    "          test_df.iloc[indx,-1] = (i_RUL + df_pivot2.Time - 1).values[::-1]\n",
    "          # el comando [::-1] nos invirte el orden del resultado\n",
    "# Convertimos las listas en numpy arrays\n",
    "list_RUL_test = np.asarray(list_RUL_test)\n",
    "list_time_test = np.asarray(list_time_test)\n",
    "\n",
    "# Eliminamos los indices con duraciones menores\n",
    "test_df.drop(index = indx_delet,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcKrMyg7b8tv"
   },
   "outputs": [],
   "source": [
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE0T563K2Ri3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.kdeplot(list_RUL_test, color= \"Blue\", shade = True)\n",
    "plt.xlabel(\"RUL\",size = 20)\n",
    "plt.ylabel(\"Frecuency\",size = 20)\n",
    "plt.title('Distribucion de RUL',size = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.kdeplot(list_time_test, color= \"Blue\", shade = True)\n",
    "plt.xlabel(\"Time-Serie\",size = 20)\n",
    "plt.ylabel(\"Frecuency\",size = 20)\n",
    "plt.title('Distribucion de Time-Serie',size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9FN7VtlwmZS"
   },
   "outputs": [],
   "source": [
    "global_df = pd.concat([train_df, test_df], join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XB7QDg6c_eu"
   },
   "outputs": [],
   "source": [
    "global_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02n3OGO2uPBO"
   },
   "source": [
    "Definiremos una nueva columna, que identifique inequivocamente la unidad a la que corresponde la serie temporal.\n",
    "\n",
    "Para eso el comando .diff() calcula la diferencia entre el valor de la fila actual y la fila anterior.\n",
    "\n",
    "Solo tendremos un valor distinto de 0 cuando haya un cambio de unidad.\n",
    "\n",
    "Cualquier valor distinto de 0 lo pasaremos a binario (0 o 1).\n",
    "\n",
    "Luego iremos completando la columna con una suma acumulada.\n",
    "\n",
    "Como la mayoria son 0, lo que iremos obteniendo en la columa \"Unidad_new\" sera el numero del equipo independientemente del archivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH-8R1A_xsAC"
   },
   "outputs": [],
   "source": [
    "# Diferencias\n",
    "global_df['Unit_new'] = global_df['Unit'].diff()\n",
    "\n",
    "# El primer registro no tiene ningun registro previo, por lo que se nos llena con un nan, el cual lo reemplazamos por 1\n",
    "global_df['Unit_new'].fillna(value=1,inplace=True)\n",
    "\n",
    "# Pasamos a booleano (solo 1 o 0) y luego a entero\n",
    "global_df['Unit_new'] = global_df['Unit_new'].astype(bool).astype(int)\n",
    "\n",
    "# Suma acumulada\n",
    "global_df['Unit_new'] = global_df['Unit_new'].cumsum()\n",
    "\n",
    "# Lista con el numero de unidad 'global' del equipo\n",
    "list_units_global = global_df['Unit_new'].unique()\n",
    "\n",
    "list_time_global = np.concatenate((list_time_train,list_time_test))\n",
    "t_max = max(list_time_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sebCJSNcc4lQ"
   },
   "outputs": [],
   "source": [
    "global_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wViY8ilspWxi"
   },
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by_BF-KDEbUq"
   },
   "source": [
    "Una vez que tenemos toda la informacion consolidada, vamos conformar, de manera aleatoria, los set de entrenamiento, validacion y testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vmg-ZTE2RjY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Proporcional nuevo set\n",
    "t_size= 0.15\n",
    "r_seed = 0\n",
    "\n",
    "# Train - Test\n",
    "list_units_train, list_units_test, list_time_train, list_time_test = train_test_split(list_units_global, list_time_global, test_size=t_size, random_state=r_seed)\n",
    "# Train - Validation\n",
    "list_units_train, list_units_val, list_time_train, list_time_val = train_test_split(list_units_train, list_time_train, test_size=t_size, random_state=r_seed)\n",
    "\n",
    "n_unidades_train = np.shape(list_units_train)[0]\n",
    "n_unidades_val = np.shape(list_units_val)[0]\n",
    "n_unidades_test = np.shape(list_units_test)[0]\n",
    "print(f'Train: {n_unidades_train} | Val: {n_unidades_val} | Test: {n_unidades_test}')\n",
    "\n",
    "# Dataframe con las unidades destinadas a cada set\n",
    "train_df = global_df[global_df.Unit_new.isin(list_units_train)].copy()\n",
    "val_df = global_df[global_df.Unit_new.isin(list_units_val)].copy()\n",
    "test_df = global_df[global_df.Unit_new.isin(list_units_test)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp9y62ZBpMBT"
   },
   "source": [
    "# AUTOSCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR475c0m2RjC"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Tipo de escalado que vamos a usar\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "#Aqui obtenemos los valores\n",
    "train = train_df[features_columns].values.astype(float)\n",
    "# Usando el metodo \"fit_transform\" le indicamos al objeto \"scaler\" que calcule los parametros para escalar, y que nos devuelva\n",
    "# los datos ya escalados\n",
    "# Si queremos acceder a los parametros del objeto, existe el metodo get_params()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "# Los datos ya normalzados los copiamos en el dataframe correspondiente\n",
    "train_df[features_columns] = train_scaled\n",
    "\n",
    "# VALIDATION\n",
    "val = val_df[features_columns].values.astype(float)\n",
    "# Ahora solo usamos el metodo \"transform()\" ya que el objeto \"scaler\" tiene\n",
    "# internamente los parametros para escalar\n",
    "val_scaled = scaler.transform(val)\n",
    "val_df[features_columns] = val_scaled\n",
    "\n",
    "# TEST\n",
    "test = test_df[features_columns].values.astype(float)\n",
    "test_scaled = scaler.transform(test)\n",
    "test_df[features_columns] = test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl5W931gIt_v"
   },
   "source": [
    "En este punto vamos a dejar de tener al informacion en Pandas Dataframe, y la vamos pasar a numpy arrays.\n",
    "\n",
    "Los inputs del modelo debe ser de la siguientes dimensiones:\n",
    "\n",
    "**[ N° Elementos , Longitud Temporal, Features ]**\n",
    "\n",
    "Pre-alojaremos con ceros los arrays n-dimensionales de los set de entramiento, validacion y testeo.\n",
    "\n",
    "Usaremos como longitud temporal a la maxima duracion de las series temporales en los datos.\n",
    "\n",
    "Las series que tengan menores duraciones seran completadas con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koJ1cvmAyGbe"
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "n_caracteristicas = len(features_columns)\n",
    "\n",
    "x_train = np.zeros((n_unidades_train, t_max,n_caracteristicas))\n",
    "y_train = np.zeros((n_unidades_train, t_max, 1),dtype=np.int)\n",
    "\n",
    "x_val = np.zeros((n_unidades_val, t_max,n_caracteristicas))\n",
    "y_val = np.zeros((n_unidades_val, t_max, 1),dtype=np.int)\n",
    "\n",
    "x_test = np.zeros((n_unidades_test, t_max,n_caracteristicas))\n",
    "y_test = np.zeros((n_unidades_test, t_max, 1),dtype=np.int)\n",
    "\n",
    "#Dimensiones de los nd-arrays\n",
    "print(np.shape(x_train),np.shape(y_train))\n",
    "print(np.shape(x_val),np.shape(y_val))\n",
    "print(np.shape(x_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOvuorCVLZYx"
   },
   "source": [
    "Ahora para cada unidad de cada uno de los tres Dataframes, copiaremos los valores en los nd-array creados en la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNfRO6cly1m0"
   },
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "for i, i_unit in enumerate(list_units_train):\n",
    "    # Features a X, RUL a Y\n",
    "    x = train_df[train_df['Unit_new']==i_unit][features_columns].values\n",
    "    y = train_df[train_df['Unit_new']==i_unit]['RUL'].values\n",
    "    # Ajustamos la dimension de Y\n",
    "    y = y[...,np.newaxis]\n",
    "    # list_time_train nos da la posicion de fin de la serie temporal \n",
    "    x_train[i,0:list_time_train[i],:] = x\n",
    "    y_train[i,0:list_time_train[i],:] = y\n",
    "#VALIDATION\n",
    "for i, i_unit in enumerate(list_units_val):\n",
    "    # Features a X, RUL a Y\n",
    "    x = val_df[val_df['Unit_new']==i_unit][features_columns].values\n",
    "    y = val_df[val_df['Unit_new']==i_unit]['RUL'].values\n",
    "    # Ajustamos la dimension de Y\n",
    "    y = y[...,np.newaxis]\n",
    "    x_val[i,0:list_time_val[i],:] = x\n",
    "    y_val[i,0:list_time_val[i],:] = y\n",
    "#TEST\n",
    "for i, i_unit in enumerate(list_units_test):\n",
    "    # Features a X, RUL a Y\n",
    "    x = test_df[test_df['Unit_new']==i_unit][features_columns].values\n",
    "    y = test_df[test_df['Unit_new']==i_unit]['RUL'].values\n",
    "    # Ajustamos la dimension de Y\n",
    "    y = y[...,np.newaxis]\n",
    "    x_test[i,0:list_time_test[i],:] = x\n",
    "    y_test[i,0:list_time_test[i],:] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11hInVAopTfh"
   },
   "source": [
    "# LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VOmNqo9Np6_"
   },
   "source": [
    "Para este ejercicio vamos a definir estados/clases en el que cada unidad se puede encontrar:\n",
    "\n",
    "Clase/ Estado:\n",
    "*   0 (Estado Critico) : Si RUL < 50\n",
    "*   1 (Mantenimiento Preventivo): Si RUL $i\\in$ [50,100]\n",
    "*   2 (Estado Normal): Si RUL > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp0F2tM4PWB3"
   },
   "outputs": [],
   "source": [
    "class0 = y_train < 50\n",
    "class1 = np.logical_and(y_train<=100, y_train>=50)\n",
    "class2 = y_train > 100\n",
    "y_train[class0]= 0\n",
    "y_train[class1]= 1\n",
    "y_train[class2]= 2\n",
    "\n",
    "class0 = y_val < 50\n",
    "class1 = np.logical_and(y_val<=100, y_val>=50)\n",
    "class2 = y_val > 100\n",
    "y_val[class0]= 0\n",
    "y_val[class1]= 1\n",
    "y_val[class2]= 2\n",
    "\n",
    "class0 = y_test < 50\n",
    "class1 = np.logical_and(y_test<=100, y_test>=50)\n",
    "class2 = y_test > 100\n",
    "y_test[class0]= 0\n",
    "y_test[class1]= 1\n",
    "y_test[class2]= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZAbxWUCOYkN"
   },
   "source": [
    "# DATA WINDOWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQTp5yfnkkEr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_generator(batch_size, X, Y,list_time,t):\n",
    "    num_examples = len(X)\n",
    "    examples = zip(X, Y, list_time)\n",
    "    examples = sorted(examples, key = lambda x: X[0].shape[0])\n",
    "    end = num_examples - batch_size + 1\n",
    "    batches = [examples[i:i + batch_size] for i in range(0, end, batch_size)]\n",
    "\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    while True:\n",
    "      for batch in batches:\n",
    "        x_b, y_b, list_time_b = zip(*batch)\n",
    "        # Pasamos a array\n",
    "        x_b = np.asarray(x_b).copy()\n",
    "        y_b = np.asarray(y_b,dtype=int).copy()\n",
    "        list_time_b = np.asarray(list_time_b,dtype=int).copy()\n",
    "        \n",
    "        # Generamos los array para enviar al modelo\n",
    "        x_batch = np.zeros((np.shape(x_b)[0],t,np.shape(x_b)[2]))\n",
    "        y_batch = np.zeros((np.shape(x_b)[0]),dtype=int)\n",
    "\n",
    "        #Loop para cada muestra / unidad\n",
    "        for index in np.arange(len(x_batch)):\n",
    "          \n",
    "          # Tiempo maximo de la serie temporal\n",
    "          t_max = list_time_b[index]\n",
    "          if t_max>t:\n",
    "            # Elegimos aleatoreamente un segmento de la serie temporal\n",
    "            t_view = np.random.randint(t,t_max)\n",
    "          else:\n",
    "            # Para evitar error cuando la serie temporal tiene la duracion minima\n",
    "            # y no habria valores aleatorios en el intervalo en cuestion.\n",
    "            t_view = t\n",
    "\n",
    "          x = x_b[index,t_view - t : t_view]\n",
    "          y = y_b[index,t_view - t : t_view]\n",
    "\n",
    "          # Ultimo valor del segmento de la serie temporal, el estado de la unidad\n",
    "          y = y[-1]\n",
    "          x_batch[index] = x\n",
    "          y_batch[index] = y\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpD4Esy7rj-5"
   },
   "outputs": [],
   "source": [
    "# Batch Sizes\n",
    "b_size_train = 48 \n",
    "\n",
    "# El tamaño del batch de val y test va a ser el tamaño del respectivo set.\n",
    "b_size_val = n_unidades_val \n",
    "b_size_test = n_unidades_test\n",
    "\n",
    "#Definimos los 3 generadores\n",
    "train_ds = data_generator(b_size,x_train,y_train,list_time_train,lim_t)\n",
    "val_ds = data_generator(b_size_val,x_val,y_val,list_time_val,lim_t)\n",
    "test_ds = data_generator(b_size_test,x_test,y_test,list_time_test,lim_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnqhvwfoFJTR"
   },
   "source": [
    "Existen otros metodos para formar las ventanas temporales, que quizas requieren mas \"complejidad\" para armarlos, pero se obtiene mejores rendimientos al entrenar, como por ejemplo:\n",
    "\n",
    "* DataGenerator (Keras) :\n",
    "\n",
    "  https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "* DataWindowing :\n",
    "\n",
    "  https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ5SY4sKpFxS"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXKk7SG0tYvC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,GRU,Bidirectional\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4g58J3IMkAB"
   },
   "source": [
    "Anteriormente vimos que podemos definir los modelos utilizando 'models.Sequential'.\n",
    "\n",
    "Una forma mucho mas versatil es utilizar la 'Functional API'.\n",
    "\n",
    "\n",
    "En este ejemplo una la misma entrada se pasa a dos capas distintas. Ademas, las salidas de las capas 'B' y 'C' son utilizadas como output del modelo.\n",
    "\n",
    "```\n",
    "# Input Layer\n",
    "inputs = Input(shape=(DIMENSION DE X))\n",
    "\n",
    "# Layer A\n",
    "x_a = Dense()(inputs)\n",
    "\n",
    "# Layer B\n",
    "x_b = Dense()(inputs)\n",
    "\n",
    "# Layer C = B + A\n",
    "x_a_b = tf.keras.layers.concatenate([x_a, x_b])\n",
    "x_c = Dense()(x_a_b)\n",
    "\n",
    "# Output LayerS\n",
    "output1 = Dense()(x_c)\n",
    "output2 = Dense()(x_b)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[inputs],\n",
    "    outputs=[output1,output2]\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVEQ_oOBgKVw"
   },
   "source": [
    "### ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIjbWpWjt31q"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_inputs = Input(shape=(lim_t,n_caracteristicas))\n",
    "\n",
    "# 1° RNN Layer\n",
    "o = Bidirectional(GRU(18, return_sequences=True))(model_inputs)\n",
    "# cada salida de esta layer, debe pasarse a la segunda capa, por lo que usamos\n",
    "# el argumento 'return_sequences=True'\n",
    "\n",
    "# 2° RNN Layer\n",
    "o = Bidirectional(GRU(12, return_sequences=False))(o)\n",
    "# en esta segunda layer RNN, como solo vamos a clasificar usando al ultima\n",
    "# salida, ingresamos 'return_sequences=False'\n",
    "\n",
    "# Output Layer\n",
    "model_outputs = Dense(3,activation='softmax')(o)\n",
    "\n",
    "model = tf.keras.Model(inputs=model_inputs, outputs=model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KWBLQ2kgRgq"
   },
   "source": [
    "### SUMMARY AND PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xTcY6LsxPUx"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omfQLaCMMeuy"
   },
   "source": [
    "# LOSS AND OPTMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O58uMQQGyGS3"
   },
   "outputs": [],
   "source": [
    "# Optimizador\n",
    "lr = 0.0005\n",
    "# clip norm es MUY importante cuando entrenamos RNN para evitar la explosion del\n",
    "# gradiente, que 'rompe' los pesos aprendidos.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr,clipnorm=3)\n",
    "# Funcion de penalizacion\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "\n",
    "model.compile(optimizer=opt, loss=loss_func,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axJAp5_RMdI2"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HoO7o18yvb1"
   },
   "outputs": [],
   "source": [
    "training = model.fit(train_ds, epochs=1000,\n",
    "                steps_per_epoch=n_unidades_train//b_size,\n",
    "                validation_data=val_ds,validation_steps=n_unidades_val//b_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIh1Ewz0T5vx"
   },
   "source": [
    "En este punto ya tenemos el modelo entrenado y podemos ver como fue el progreso del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn0ArzshnfS2"
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "loss_history = training.history['loss']\n",
    "val_loss_hist = training.history['val_loss']\n",
    "epochs = range(1, len(loss_history) + 1)\n",
    "plt.plot(epochs, loss_history, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss_hist, 'b', label='Validation loss')\n",
    "plt.title('Training and validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Accuracy\n",
    "acc_history = training.history['accuracy']\n",
    "val_acc_hist = training.history['val_accuracy']\n",
    "epochs = range(1, len(acc_history) + 1)\n",
    "plt.plot(epochs, acc_history, 'r', label='Training Acc.')\n",
    "plt.plot(epochs, val_acc_hist, 'b', label='Validation Acc.')\n",
    "plt.title('Training and validation Acc.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHe4xDwcUGYC"
   },
   "source": [
    "El entrenamiento se ve muy \"ruidoso\", por lo que vamos a suavizarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpJkCa3VDKHA"
   },
   "outputs": [],
   "source": [
    "# Suavizar\n",
    "def curva_suavizada(puntos, factor):\n",
    "    # En esta variable iremos guardando puntos\n",
    "    puntos_suavizados = []\n",
    "    for punto in puntos:\n",
    "        if puntos_suavizados:\n",
    "            anterior = puntos_suavizados[-1]\n",
    "            puntos_suavizados.append(anterior * factor + punto * (1 - factor))\n",
    "        else:\n",
    "            puntos_suavizados.append(punto)\n",
    "    return puntos_suavizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H6e9_uDDYsY"
   },
   "outputs": [],
   "source": [
    "# Factor de suavizado\n",
    "factor = 0.90\n",
    "plt.plot(curva_suavizada(loss_history, factor),color='blue',label='Train')\n",
    "plt.plot(curva_suavizada(val_loss_hist, factor),color='red',label='Validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(curva_suavizada(acc_history, factor),color='blue',label='Train')\n",
    "plt.plot(curva_suavizada(val_acc_hist, factor),color='red',label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCK4MbSXtPW2"
   },
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR2Au41rbOQR"
   },
   "source": [
    "Finalmente vamos a evaluar sobre el set de testeo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njh6e04pHfjh"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds,steps=n_unidades_test//b_size_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCweRwysMZbp"
   },
   "source": [
    "# PREDICTION\n",
    "\n",
    "Guardaremos para cada serie temporal, el valor de verdadero (y_true) y el valor predicho (y_pred).\n",
    "\n",
    "Como vimos en clases anteriores, la salida de nuestra red cuando clasificamos es un one-hot vector con las 'probabilidades' de cada muestra (ya que usamos la funcion **softmax** en la ultima capa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US9prDWWJ8Bj"
   },
   "outputs": [],
   "source": [
    "# Generamos una ventana como muestra\n",
    "x_to_pred, y_true = next(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYsnxc_4IkSO"
   },
   "outputs": [],
   "source": [
    "# Predecimos\n",
    "y_hat = model.predict(x = x_to_pred) # Salida de la red (%)\n",
    "y_pred = np.argmax(y_hat, axis=1) # Clase de la salida (Clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiXjrNv5MPuJ"
   },
   "source": [
    "# CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KdvknWxlSL9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(c_matrix, annot=True, ax = ax)\n",
    "ax.set_xlabel('Prediccion')\n",
    "ax.set_ylabel('Verdadero')\n",
    "ax.set_title('Matriz de Confusion')\n",
    "ax.xaxis.set_ticklabels(['Critico', 'Preventivo','Normal'])\n",
    "ax.yaxis.set_ticklabels(['Critico', 'Preventivo','Normal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOuESlkFRoxf"
   },
   "source": [
    "# CONSULTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDL-Aj-Jlm7o"
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#------------ CONSULTAS -----------------#\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYKk8h1Rj2iP"
   },
   "source": [
    "# PROPUESTAS:\n",
    "\n",
    "- **FEATURE SELECTION**:\n",
    "\n",
    "    * Hemos usado **TODOS** los sensores, y los 3 modos de operacion, pero no analizamos si agregan o no informacion.\n",
    "\n",
    "- **AUTOSCALING**:\n",
    "  * El escalado que hicimos fue MinMax por cada sensor.Comparar con normalizacion.\n",
    "\n",
    "- **CROSS-VALIDATION**:\n",
    "  * Hacer una validacion cruzada 5 veces. Hasta ahora, cuando hicimos Cross-Validation, usamos 'GridSearchCV' de la libreria Sk-learn. Las librerias de deep learning, **NO** tienen integrada esta funcionalidad. Programar\n",
    "\n",
    "- **REGRESION**:\n",
    "\n",
    "  * En vez de clasificar los estados de las turbinas, estimen el valor de RUL.\n",
    "\n",
    "  Ayuda:\n",
    "\n",
    "  ```\n",
    "  Arquitectura:\n",
    "  model_outputs = Dense(1)\n",
    "\n",
    "  Loss:\n",
    "  loss = tf.keras.losses.MSE\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVSiwsSJsz0s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "clusterai2020_clase12_RNN_Clasifacion.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
